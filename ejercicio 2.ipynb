# utilizado para la manipulación de directorios y rutas
import os

# Cálculo científico y vectorial para python
import numpy as np

# Libreria para graficos
from PIL import Image
from matplotlib import pyplot

# Modulo de optimizacion en scipy
from scipy import optimize

# modulo para cargar archivos en formato MATLAB
from scipy.io import loadmat

# le dice a matplotlib que incruste gráficos en el cuaderno
%matplotlib inline
import scipy.io

# Cargar los archivos .mat en la lista all_data conteniendo todos los archivos.mat
all_data = []
file_names = ['emnist-digits.mat', 'emnist-balanced.mat', 'emnist-byclass.mat', 'emnist-letters.mat', 'emnist-bymerge.mat', 'emnist-mnist.mat']

for file_name in file_names:
    data = scipy.io.loadmat(file_name)
    all_data.append(data)
    print("Claves disponibles en el archivo '{}':".format(file_name))
    print(data.keys())
    print()
#se carga a data los datos que estan en filenames archivo por archivo como tambien se muestra las 
#llaves de cada archivo 

# Mostrar información de los datos de entrenamiento y prueba
# Mediante el for se sabe el todos los datos de entrenamiento , las imagenes de pruebas y etiquetas
for i, data in enumerate(all_data):
    dataset = data['dataset'][0][0]
    train_data = dataset['train'][0][0]
    test_data = dataset['test'][0][0]
    
    print("Información del archivo '{}':".format(file_names[i]))
    print("Datos de entrenamiento:")
    print(" - Imágenes de entrenamiento: ", train_data['images'].shape)
    print(" - Etiquetas de entrenamiento: ", train_data['labels'].shape)
    print("Datos de prueba:")
    print(" - Imágenes de prueba: ", test_data['images'].shape)
    print(" - Etiquetas de prueba: ", test_data['labels'].shape)
    print()


dataset = data['dataset']  # se entra al conjunto de datos de data con la clave dataset
train_data = dataset[0][0]['train'] # se entra al conjunto de train datos de data con la clave train
test_data = dataset[0][0]['test'] # se entra al conjunto de test datos de data con la clave test

# Arreglo de imágenes para entrenamiento
images_train = train_data[0][0]['images']

# Obtener el número de dígitos en la imagen de entrenamiento usando una dimension "0"
num_digits = images_train.shape[0]

print("Número de dígitos en la imagen:", num_digits)

train_data = dataset[0][0]['train'] # nuevamente se accede con la clave train y la de test
test_data = dataset[0][0]['test']

train_images = train_data['images'][0, 0]  # ahora se accede a las imagenes almacenadas en estas 
test_images = test_data['images'][0, 0]

train_digit_size = train_images.shape # se toma la dimension de cada imagen de entrenamiento y de prueba
test_digit_size = test_images.shape

train_num_pixels = np.prod(train_digit_size) # se calcula lacantidad de pixeles usados en cada imagen 
test_num_pixels = np.prod(test_digit_size)

print("Tamaño de cada dígito de entrenamiento:", train_digit_size)
print("Número total de píxeles por dígito de entrenamiento:", train_num_pixels)

print("Tamaño de cada dígito de prueba:", test_digit_size)
print("Número total de píxeles por dígito de prueba:", test_num_pixels)

import matplotlib.pyplot as plt
import random

# Obtener los datos de todos los archivos .mat almacenados en all data
all_data = []
file_names = ['emnist-digits.mat', 'emnist-balanced.mat', 'emnist-byclass.mat', 'emnist-letters.mat', 'emnist-bymerge.mat', 'emnist-mnist.mat'] 

for file_name in file_names:
    data = loadmat(file_name) # pone todo en data nuevamente
    all_data.append(data)

# Configuración de la cuadrícula que contiene los numeros y letras 
num_rows = len(file_names)  # Número de filas en la cuadrícula 
num_cols = 5  # Número de columnas en la cuadrícula
fig, axs = plt.subplots(num_rows, num_cols, figsize=(12, 10)) # se crea una figura de subplot con cuadricula de dimensiones establecidas

# Mostrar imágenes aleatorias de cada conjunto de datos
for i, file_name in enumerate(file_names):
    data = all_data[i]
    dataset = data['dataset']
    train_data = dataset[0][0]['train']
    train_images = train_data['images'][0, 0]
    train_labels = train_data['labels'][0, 0]
    
    # Seleccionar aleatoriamente imágenes para mostrar
    random_indices = random.sample(range(train_images.shape[0]), num_cols)
    
    for j, index in enumerate(random_indices): 
        img = train_images[index].reshape(28, 28).T  # Ajustar las dimensiones de la imagen para que sean 28x28
        label = train_labels[index][0]  # Obtener el valor de etiqueta de cada imagen obtenida
        axs[i, j].imshow(img, cmap='gray')  # muestra la imagen en su posicion adecuada 
        axs[i, j].set_title('Archivo: {}\nEtiqueta: {}'.format(file_name, label))
        axs[i, j].axis('off')

plt.tight_layout()
plt.show()


print("Dimensiones de las imágenes:", train_images[0].shape) # muestra la dimension de la imagen de entrenamiento
import scipy.io

# Archivos .mat a cargar en file names
file_names = ['emnist-digits.mat', 'emnist-balanced.mat', 'emnist-byclass.mat', 'emnist-letters.mat', 'emnist-bymerge.mat', 'emnist-mnist.mat']

# Listas para almacenar los datos en la lista
X_list = []
y_list = []

# Cargar los datos de cada archivo .mat
for file_name in file_names:
    data = scipy.io.loadmat(file_name)
    dataset = data['dataset']
    train_data = dataset[0][0]['train']
    train_images = train_data['images'][0, 0]
    train_labels = train_data['labels'][0, 0]
    num_images = train_images.shape[0]
    image_size = train_images.shape[1]
    # recorre todos los datos de entrenamiento y los almacena en X y Y para ponerlos en las listas 
    X = train_images.reshape(num_images, image_size)
    y = train_labels.ravel()
    X_list.append(X)
    y_list.append(y)

# Combinar todos los datos de entrenamiento y prueba 
X = np.concatenate(X_list, axis=0)
y = np.concatenate(y_list, axis=0)
m = y.size

import numpy as np

#imprimimos para cargar y combinar los datos en X e y

print("Valores de X:")
print(X)
print("\nValores de y:")
print(y)
import random

# Obtén los datos originales solo de 1 libreria para no sobrecargar los datos  ni la ram ni mi memoria
data = loadmat('emnist-digits.mat')
dataset = data['dataset']
train_data = dataset[0][0]['train']
train_images = train_data['images'][0, 0]
train_labels = train_data['labels'][0, 0]
 
#Basicamente estamos reduciendo la cantidad de datos por que sino la pc tarda 90 dias en leerlo
# Crear la matriz X y el vector y
num_images = train_images.shape[0]
image_size = train_images.shape[1]
X = train_images.reshape(num_images, image_size)
y = train_labels.ravel()

# Especifica el tamaño de la muestra deseada para los datos 
sample_size = 1000

# Obtén una muestra aleatoria de índices en y
indices = random.sample(range(X.shape[0]), sample_size)

# Selecciona los datos correspondientes a los índices de muestra
X_sample = X[indices]
y_sample = y[indices]
def displayData(X, example_width=None, figsize=(10, 10)):
    """
    Muestra datos 2D almacenados en X en una cuadrícula apropiada.
    """
    # Calcula filas, columnas
    if X.ndim == 2:
        m, n = X.shape
    elif X.ndim == 1:
        n = X.size
        m = 1
        X = X[None]  # Promocionar a una matriz bidimensional
    else:
        raise IndexError('La entrada X debe ser 1 o 2 dimensinal.')

    example_width = example_width or int(np.round(np.sqrt(n)))
    example_height = n / example_width

    # Calcula el numero de elementos a mostrar
    display_rows = int(np.floor(np.sqrt(m)))
    display_cols = int(np.ceil(m / display_rows))

    fig, ax_array = pyplot.subplots(display_rows, display_cols, figsize=figsize)
    fig.subplots_adjust(wspace=0.025, hspace=0.025)

    ax_array = [ax_array] if m == 1 else ax_array.ravel()

    for i, ax in enumerate(ax_array):
        ax.imshow(X[i].reshape(example_width, example_width, order='F'),
                  cmap='Greys', extent=[0, 1, 0, 1])
        ax.axis('off')
# Basicamente la funcion display pone todo en un solo plano para su mejor visualizacion 
# osease para saber en que pixel estamos trabajando o con cual indice para su reconocimiento


# Obtén una muestra reducida de los datos
sample_size = 100  # Tamaño de la muestra deseada
indices = random.sample(range(X.shape[0]), sample_size)
X_sample = X[indices]

# Muestra los datos utilizando la función displayData
displayData(X_sample)
# valores de prueba para los parámetros theta
theta_t = np.array([-2, -1, 1, 2], dtype=float)

# valores de prueba para las entradas
X_t = np.concatenate([np.ones((5, 1)), np.arange(1, 16).reshape(5, 3, order='F')/10.0], axis=1)
print(X_t)
# valores de testeo para las etiquetas
y_t = np.array([1, 0, 1, 0, 1])

# valores de testeo para el parametro de regularizacion
lambda_t = 3
def sigmoid(z):
    """
    Calcula la sigmoide de z.
    """
    return 1.0 / (1.0 + np.exp(-z))
def lrCostFunction(theta, X, y, lambda_):
    """
    Calcula el costo de usar theta como parámetro para la regresión logística regularizada y 
    el gradiente del costo w.r.t. a los parámetros.
    
    Parametros
    ----------
    theta : array_like
        Parametro theta de la regresion logistica. Vector de la forma(shape) (n, ). n es el numero de caracteristicas 
        incluida la intercepcion
        
    X : array_like
        Dataset con la forma(shape) (m x n). m es el numero de ejemplos, y n es el numero de 
        caracteristicas (incluida la intercepcion).
    
    y : array_like
        El conjunto de etiquetas. Un vector con la forma (shape) (m, ). m es el numero de ejemplos
    
    lambda_ : float
        Parametro de regularización. 
    
    Devuelve
    -------
    J : float
        El valor calculado para la funcion de costo regularizada. 
    
    grad : array_like
        Un vector de la forma (shape) (n, ) que es el gradiente de la 
        función de costo con respecto a theta, en los valores actuales de theta..
    """
    # Inicializa algunos valores utiles
    m = y.size
    
    # convierte las etiquetas a valores enteros si son boleanos
    if y.dtype == bool:
        y = y.astype(int)
    
    J = 0
    grad = np.zeros(theta.shape)
    
    h = sigmoid(X.dot(theta.T))
    
    temp = theta
    temp[0] = 0
    
    J = (1 / m) * np.sum(-y.dot(np.log(h)) - (1 - y).dot(np.log(1 - h))) + (lambda_ / (2 * m)) * np.sum(np.square(temp))
    
    grad = (1 / m) * (h - y).dot(X) 
    grad = grad + (lambda_ / m) * temp

    return J, grad
J, grad = lrCostFunction(theta_t, X_t, y_t, lambda_t)

print('Costo         : {:.6f}'.format(J))
print('Costo esperadot: 2.534819')
print('-----------------------')
print('Gradientes:')
print(' [{:.6f}, {:.6f}, {:.6f}, {:.6f}]'.format(*grad))
print('Gradientes esperados:')
print(' [0.146561, -0.548558, 0.724722, 1.398003]');
def oneVsAll(X, y, num_labels, lambda_):
    """
    Trains num_labels logistic regression classifiers and returns
    each of these classifiers in a matrix all_theta, where the i-th
    row of all_theta corresponds to the classifier for label i.
    
    Parameters
    ----------
    X : array_like
        The input dataset of shape (m x n). m is the number of 
        data points, and n is the number of features. Note that we 
        do not assume that the intercept term (or bias) is in X, however
        we provide the code below to add the bias term to X. 
    
    y : array_like
        The data labels. A vector of shape (m, ).
    
    num_labels : int
        Number of possible labels.
    
    lambda_ : float
        The logistic regularization parameter.
    
    Returns
    -------
    all_theta : array_like
        The trained parameters for logistic regression for each class.
        This is a matrix of shape (K x n+1) where K is number of classes
        (ie. `numlabels`) and n is number of features without the bias.
    """
    # algunas variables utiles
    m, n = X.shape
    
    all_theta = np.zeros((num_labels, n + 1))

    # Agrega unos a la matriz X
    X = np.concatenate([np.ones((m, 1)), X], axis=1)

    for c in np.arange(num_labels):
        initial_theta = np.zeros(n + 1)
        options = {'maxiter': 50}
        res = optimize.minimize(lrCostFunction, 
                                initial_theta, 
                                (X, (y == c), lambda_), 
                                jac=True, 
                                method='CG',
                                options=options) 
        
        all_theta[c] = res.x

    return all_theta
#El one for all pues basicamente a los clasificadores de regresion por cada posible etiqueta 

lambda_ = 0.1
num_labels = len(np.unique(y))
all_theta = oneVsAll(X, y, num_labels, lambda_)
def predictOneVsAll(all_theta, X):
    """
    Devuelve un vector de predicciones para cada ejemplo en la matriz X.
    Tenga en cuenta que X contiene los ejemplos en filas. 
    all_theta es una matriz donde la i-ésima fila es un vector theta de regresión logística entrenada para la i-ésima clase. 
    Debe establecer p en un vector de valores de 0..K-1 (por ejemplo, p = [0, 2, 0, 1] 
    predice clases 0, 2, 0, 1 para 4 ejemplos).
    
    Parametros
    ----------
    all_theta : array_like
        The trained parameters for logistic regression for each class.
        This is a matrix of shape (K x n+1) where K is number of classes
        and n is number of features without the bias.
    
    X : array_like
        Data points to predict their labels. This is a matrix of shape 
        (m x n) where m is number of data points to predict, and n is number 
        of features without the bias term. Note we add the bias term for X in 
        this function. 
    
    Devuelve
    -------
    p : array_like
        The predictions for each data point in X. This is a vector of shape (m, ).
    """
    
    m = X.shape[0];
    num_labels = all_theta.shape[0]

    p = np.zeros(m)

    # Add ones to the X data matrix
    X = np.concatenate([np.ones((m, 1)), X], axis=1)
    p = np.argmax(sigmoid(X.dot(all_theta.T)), axis = 1)

    return p
import random

# Especifica el tamaño de la muestra deseada , esto es para estar seguros que estamos trabajando con datos reducidos
sample_size = 1000

# Obtén una muestra aleatoria de índices
indices = random.sample(range(X.shape[0]), sample_size)

# Selecciona los datos correspondientes a los índices de muestra
X_sample = X[indices]
y_sample = y[indices]

print(X.shape)
pred = predictOneVsAll(all_theta, X)
print('Precision del conjuto de entrenamiento: {:.2f}%'.format(np.mean(pred == y) * 100))
XPrueba = X[4002:4003, :].copy()
print(XPrueba.shape)
#print(np.ones((1)))
#print(XPrueba)
#p = np.zeros(1)
XPrueba = np.concatenate([np.ones((1, 1)), XPrueba], axis=1)
print(XPrueba.shape)
p = np.argmax(sigmoid(XPrueba.dot(all_theta.T)), axis = 1)
print(p)

displayData(X[4002:4003, :])
